{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Support Vector Machines on PYNQ\n",
    "\n",
    "---\n",
    "## Ryan Greer, University of Strathclyde\n",
    "\n",
    "Xilinx Open Hardware 2020 Submission\n",
    "\n",
    "---\n",
    "Compatible with PYNQ-Z2 and PYNQ v2.5\n",
    "\n",
    "---\n",
    "<img src=\"logo_banner.png\" length=250 width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "---\n",
    "This notebook demonstrates my OpenHW 2020 submission which is the Hardware Acceleration of Support Vector Machines on PYNQ.\n",
    "\n",
    "This was also my 4th Year self-proposed project dissertation.\n",
    "\n",
    "The dataset used for this demo is derived from hyperspectral images of antibiotic-producing bacteria and I was inspired by a University of Strathclyde research project (VIP) which seeks to use classification algorithms to quickly identify properties of bacteria strains from their spectral data, i.e. from hyperspectral images. This work aims to accelerate the process of drug discovery, helping the fight against antibiotic resistance. My project investigated using FPGA and SoC hardware to accelerate the latency performance of one such classification algorithm, Support Vector Machines or SVM. Both SVM training (using known data to develop a training model) and deployment (using this model to classify unknown data) algorithms have been accelerated and the results, which are largely positive, are presented at the end of this notebook and in my report.\n",
    "\n",
    "This classifier is general and could be used to classify any data. I've identified other areas where this research would be useful, particularly in space-based image processing systems; more details may be found in my report.\n",
    "\n",
    "Please run through the cells in order to see an interactive demo of the project using a real hyperspectral dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Background\n",
    "---\n",
    "A hyperspectral camera captures spectral data at a large number of wavelengths, usually hundreds or thousands of distinct wavelengths; for our data we usually keep 256 of these. This means each pixel of the image has 256 values and we can think of a hyperspectral image as a \"data cube\", an illustration of which is given in Figure 1:\n",
    "<br>\n",
    "<img src=\"hs_data_cube.png\" length=350 width=350>\n",
    "<br>\n",
    "<center> <u> Figure 1: Illustration of Hyperspectral Data Cube [1] </u> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this project, I have already pre-processed the data:\n",
    "\n",
    "I have isolated the bacteria (area of interest) from the background - we do not wish to train a model using the background pixels and there is no need to classify the background.\n",
    "\n",
    "The remaining pixels have then been re-arranged into a \"data matrix\" format, where the rows represent pixels (observations) and the columns represent wavelengths (variables). We can think of this data matrix as a series of points or vectors in \"n-dimensional\" space, where each point represents a pixel and 'n' is the number of wavelengths.\n",
    "\n",
    "SVM uses this space to draw \"optimally separating hyperplanes\" between known pairs of classes to produce the training model. Then new points, which are unknown, can be classified based on their position relative to these hyperplanes. This is illustrated later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hardware Design - Brief Overview\n",
    "---\n",
    "This section gives a brief overview of my hardware design which I used Vivado High-Level Synthesis (HLS) and Vivado IP Integrator to create.\n",
    "\n",
    "As mentioned, the purpose of my project was to accelerate the latency performance of the SVM algorithm, both training and deployment. This has been achieved in two key ways:\n",
    "- Through pipeline directives applied to accumulator loops within the HLS design (IP cores).\n",
    "- Through multiple copies of the IP cores operating fully in parallel to generate training models (or deploy them) for multiple pairs of classes in parallel.\n",
    "\n",
    "Figure 2, for example, illustrates the hardware design for SVM deployment. The IP cores along with their associated DMA IP for each AXI-stream port are placed in Vivado IP Integrator hierarchies then multiple copies of these hierarchies are instantiated to faciliate parallel processing of the different training models. The AXI-stream interfaces send and receive data from the processing system, e.g. for deployment they would receive the training model and send back the predicted classes:\n",
    "<br>\n",
    "<img src=\"deployment_hw_design.png\" length=450 width=450>\n",
    "<br>\n",
    "<center> <u> Figure 2: SVM Deployment High-Level Hardware Illustration </u> </center>\n",
    "<br>\n",
    "Within the hardware design, I also made use of HLS's arbitrary precision and fixed-point libraries to improve performance. I chose fixed formats which suited the dataset - hyperspectral data is usually normalised between 0 and 1 so I could use 1 integer bit and 15 fractional bits to represent the data, for example. This would need to be updated for different kinds of dataset however all other parts of the design are general for any datasets.\n",
    "\n",
    "Of course the software aspect of this project used the PYNQ environment which provided me an intuitive way to interact with my AXI-stream interfaces (through the DMA and Allocate libraries) and to present the results.\n",
    "\n",
    "Overall this project demonstrated the power of the high-level design flow offered by Vivado HLS and the PYNQ environment. This would not have been possible in such a short time-frame with traditional FPGA design methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Dataset\n",
    "---\n",
    "This demo uses a dataset with 3 hyperspectral images of 3 strains of the streptomyces bacteria, which is common for producing antibiotics. The training data (known classes) and testing data (unknown classes) are shown in Figure 3:\n",
    "<br>\n",
    "<img src=\"dataset.png\" length=500 width=500>\n",
    "<br>\n",
    "<center> <u> Figure 3: Training and Testing Data in RGB Representation </u> </center>\n",
    "<br>\n",
    "These images look distinguishable to the naked eye but this is usually not the case and this is why hyperspectral imaging is particularly useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Demonstration\n",
    "---\n",
    "### 5.1 Visualise Training Dataset\n",
    "Firstly we load the training data and visualise it. The data originally had 256 wavelengths (variables) however I used principle components analysis (PCA) to compress this to 3 dimensions for visualisation purposes. PCA is also a useful pre-processing technique for reducing latency as there is substantially less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_matrix():\n",
    "    # get training matrix (known data) from .dat file\n",
    "    f = open(\"training_matrix.dat\",\"r\")\n",
    "\n",
    "    contents = f.read()\n",
    "    training_mat_data = contents.split()\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return training_mat_data\n",
    "    \n",
    "def get_testing_matrix():\n",
    "    # get testing matrix (unknown data) from .dat file\n",
    "    f = open(\"test_matrix.dat\",\"r\")\n",
    "\n",
    "    contents = f.read()\n",
    "    testing_mat_data = contents.split()\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return testing_mat_data\n",
    "    \n",
    "    \n",
    "#%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_mat_data = get_training_matrix()\n",
    "training_plot_data = np.transpose(np.reshape(training_mat_data,(150,3)))\n",
    "training_plot_data_new = training_plot_data.tolist()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "training_plot_data_x_1 = [float(i) for i in training_plot_data_new[0][0:50]]\n",
    "training_plot_data_x_2 = [float(i) for i in training_plot_data_new[0][50:100]]\n",
    "training_plot_data_x_3 = [float(i) for i in training_plot_data_new[0][100:150]]\n",
    "\n",
    "training_plot_data_y_1 = [float(i) for i in training_plot_data_new[1][0:50]]\n",
    "training_plot_data_y_2 = [float(i) for i in training_plot_data_new[1][50:100]]\n",
    "training_plot_data_y_3 = [float(i) for i in training_plot_data_new[1][100:150]]\n",
    "\n",
    "training_plot_data_z_1 = [float(i) for i in training_plot_data_new[2][0:50]]\n",
    "training_plot_data_z_2 = [float(i) for i in training_plot_data_new[2][50:100]]\n",
    "training_plot_data_z_3 = [float(i) for i in training_plot_data_new[2][100:150]]\n",
    "\n",
    "ax.scatter(training_plot_data_x_1, training_plot_data_y_1, training_plot_data_z_1, c='red', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_2, training_plot_data_y_2, training_plot_data_z_2, c='green', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_3, training_plot_data_y_3, training_plot_data_z_3, c='blue', s=1, alpha=1)\n",
    "\n",
    "\n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the training data is 3D space with the classes indicated by colour. They form distinct clusters which means SVM should be able to generate a good training model.\n",
    "\n",
    "### 5.2 Load the SVM Training Bitstream and Driver Class\n",
    "\n",
    "We load the SVM training bitstream (or overlay) using PYNQ's \"Overlay\" class and setup the driver which is a large class containing all the functions required to parse files containing the data (and other information pertaining to the dataset) and stimulate the design and receive the result. This driver is quite complex as it is able to interact with multiple IP cores to generate the training models in parallel using sequential minimal optimisation (SMO) - note a training model is required for each pair of classes to achieve the full training model and thus there are:\n",
    "\n",
    "$ \\:\\:\\:\\:\\: kC2 = \\frac{1}{2}k(k-1) = (k-1) + (k-2)\\:+\\: ...\\: +\\: 1 $\n",
    "\n",
    "training models for $k$ classes, where $C$ is the \"choose\" operator. For example, for 3 classes there will be 3 binary training models and for 4 classes there will be 6 binary training models and so on. Therefore being able to process these binary training models in parallel (generating them for training and using them for deployment) maps very well to the FPGA's parallel architecture.\n",
    "\n",
    "The driver class also has functions to setup the contiguous heap memory for the DMA transfers and to save the resulting training models to files, for use in the SVM deployment section of the demo. There are also functions to convert from fixed point (bits read as signed integer) to float and from IEEE754 single-precision format to float in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP load the overlay\n",
    "from pynq import Overlay\n",
    "\n",
    "overlay = Overlay(\"SMO_FULL_PYNQ_Z2.bit\")\n",
    "\n",
    "from pynq import DefaultIP\n",
    "import numpy as np\n",
    "\n",
    "class parse_files():\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        # dot product matrix will be computed from two training matrix streams \n",
    "        self.training_labels_data_fi_uint8 = None\n",
    "        self.training_mat_data_fi_uint16 = None\n",
    "        self.input_details_data_fi_uint32 = None\n",
    "        \n",
    "        # miscellaneous variables\n",
    "        self.no_training_vectors_fi_uint32 = None\n",
    "        self.no_training_vectors_int = None\n",
    "        self.no_variables_fi_uint32 = None\n",
    "        self.no_variables_int = None\n",
    "        self.C_fi_uint32 = None\n",
    "        self.tolerance_fi_uint32 = None\n",
    "        # number of classifiers:\n",
    "        self.no_classes = None\n",
    "        \n",
    "    def get_training_labels(self):\n",
    "        # for self checking Python tests\n",
    "        f = open(\"training_labels.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        training_labels_data = contents.split()\n",
    "        x = np.array(training_labels_data)\n",
    "        self.training_labels_data_fi_uint8 = np.asarray(x,np.uint8)\n",
    "\n",
    "        f.close()    \n",
    "        \n",
    "    def get_training_matrix(self):\n",
    "        f = open(\"training_matrix_fi.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        training_mat_data = contents.split()\n",
    "        x = np.array(training_mat_data)\n",
    "        self.training_mat_data_fi_uint16 = np.asarray(x,np.uint16)\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "    def get_input_details(self):\n",
    "        f = open(\"training_details.dat\",\"r\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        input_details_data = contents.split()\n",
    "        x = np.array(input_details_data)\n",
    "        self.input_details_data_float = np.asfarray(x,np.float32)\n",
    "        \n",
    "        self.no_training_vectors_float = self.input_details_data_float[0]\n",
    "        self.no_training_vectors_int = int(self.no_training_vectors_float)\n",
    "        self.no_variables_float = self.input_details_data_float[1]\n",
    "        self.no_variables_int = int(self.no_variables_float)\n",
    "        self.C = self.input_details_data_float[2]\n",
    "        self.tolerance = self.input_details_data_float[3]\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        f = open(\"training_details_fi.dat\",\"r\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        input_details_data = contents.split()\n",
    "        x = np.array(input_details_data)\n",
    "        self.input_details_data_fi_uint32 = np.asarray(x,np.uint32)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    def get_no_classes(self):\n",
    "        f = open(\"no_classes.dat\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        self.no_classes = contents.split()\n",
    "        self.no_classes = int(self.no_classes[0])\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "import pynq.lib.dma\n",
    "import struct\n",
    "\n",
    "from pynq import allocate\n",
    "\n",
    "class SMO_driver(parse_files):\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.get_no_classes()\n",
    "        self.no_classifiers = int(0.5 * self.no_classes * (self.no_classes - 1))\n",
    "        \n",
    "        # DECLARE MEMORY FOR STACK\n",
    "        self.training_labels_buffer = None\n",
    "        self.training_matrix_buffer = None\n",
    "        self.input_details_buffer = None\n",
    "        self.alpha_out_buffer = None\n",
    "        self.output_details_buffer = None\n",
    "\n",
    "        # DMA library instantiate\n",
    "        # these lists contain the DMA instances for the different cores used in the overlay\n",
    "        self.dma_dp_o_1 = []\n",
    "        self.dma_dp_i_1 = []\n",
    "        self.dma_dp_o_2 = []\n",
    "        self.dma_dp_i_2 = []\n",
    "        self.dma_dp_id_o = []\n",
    "        self.dma_dp_id_i = []\n",
    "        self.dma_tl = []\n",
    "        self.dma_tm_o = []\n",
    "        self.dma_tm_i = []\n",
    "        self.dma_id = []\n",
    "\n",
    "        self.dma_ao = []\n",
    "        self.dma_kkt = []\n",
    "        self.dma_od = []\n",
    "        \n",
    "        self.no_cores = 2\n",
    "        self.classifier_indices = []\n",
    "        self.no_training_vectors_all = np.zeros(shape=(self.no_cores), dtype=np.uint32)\n",
    "        \n",
    "        # create a dispatcher to allow streamlined access to the different cores in the design\n",
    "        SMO_1 = overlay.SMO_1\n",
    "        SMO_2 = overlay.SMO_2\n",
    "        \n",
    "        self.SMO_dispatcher = {\n",
    "            1: SMO_1,\n",
    "            2: SMO_2,\n",
    "        }\n",
    "        \n",
    "        # store training models\n",
    "        self.sv_coeffs = []\n",
    "        self.sv_indices = []\n",
    "        self.no_svs = []\n",
    "        self.offsets = []\n",
    "        self.no_itrs = []    \n",
    "        \n",
    "    def fixed_point_to_float(self, input_, word_length, integer_length):\n",
    "        # returns floating point representation of fixed point SIGNED integer input\n",
    "        # specify the word length and integer length\n",
    "        \n",
    "        fractional_length = word_length - integer_length\n",
    "        output = 0\n",
    "\n",
    "        input_bin_string = \"{0:b}\".format(input_)\n",
    "\n",
    "        for n1 in range(word_length - len(input_bin_string)):\n",
    "            input_bin_string = '0' + input_bin_string\n",
    "    \n",
    "        no_positive = 1\n",
    "\n",
    "        # number is negative\n",
    "        if(input_bin_string[0] == '1'):\n",
    "            no_positive = 0\n",
    "            input_bin_tc = input_ - (1 << word_length)\n",
    "            # input is now negative\n",
    "            input_ = -input_bin_tc\n",
    "    \n",
    "        input_bin_string = \"{0:b}\".format(input_)\n",
    "\n",
    "        for n1 in range(word_length - len(input_bin_string)):\n",
    "            input_bin_string = '0' + input_bin_string\n",
    "    \n",
    "        for i, c in enumerate(input_bin_string):\n",
    "            if(c == '1'):\n",
    "                output = output + 2 ** (integer_length - 1 - i)\n",
    "\n",
    "        if(no_positive == 1):\n",
    "            return output\n",
    "        else:\n",
    "            return -output\n",
    "\n",
    "        # https://stackoverflow.com/questions/699866/python-int-to-binary-string\n",
    "        # https://stackoverflow.com/questions/538346/iterating-each-character-in-a-string-using-python\n",
    "        # https://stackoverflow.com/questions/1604464/twos-complement-in-python\n",
    "        \n",
    "    def int_bits_IEEE754_to_float(self, to_convert):\n",
    "        # credit - https://stackoverflow.com/questions/30124608/convert-unsigned-integer-to-float-in-python\n",
    "        # convert integer bits (unsigned long 'L') (IEEE754 single-precision) to float 'f'\n",
    "        s = struct.pack('>L', to_convert)\n",
    "        return struct.unpack('>f', s)[0]\n",
    "        \n",
    "    def write_training_model_to_files(self, current_classifier):  \n",
    "        # SV COEFFS TO FILE #\n",
    "        # create new numpy array to copy to file - copt pynq buffer into\n",
    "        coeffs_write = np.zeros(shape=(len(self.sv_coeffs[current_classifier-1]),1), dtype=np.uint32)\n",
    "        np.copyto(coeffs_write, self.sv_coeffs[current_classifier-1])\n",
    "        np.savetxt(\"coeffs_fi_\"+str(current_classifier)+\".dat\",coeffs_write,'%d')\n",
    "        \n",
    "        # SUPPORT VECTORS TO FILE #\n",
    "        svs_write = np.reshape(self.training_mat_data_fi_uint16,((int(len(self.training_mat_data_fi_uint16)/self.no_variables_int),self.no_variables_int)))\n",
    "        svs_write = svs_write[self.sv_indices[current_classifier-1]]\n",
    "        np.savetxt(\"svs_fi_\"+str(current_classifier)+\".dat\",svs_write,'%d')\n",
    "        \n",
    "        # OFFSET TO FILE #\n",
    "        np.savetxt(\"offset_fi_\"+str(current_classifier)+\".dat\",self.offsets[current_classifier-1],'%d')\n",
    "        \n",
    "    def write_n_svs_to_file(self):\n",
    "        # populate number of support vectors to array and write to file\n",
    "        no_classifiers = int(0.5 * self.no_classes * (self.no_classes - 1))\n",
    "        n_svs = np.zeros(shape=(no_classifiers), dtype=np.uint32)\n",
    "        \n",
    "        for n1 in range(no_classifiers):\n",
    "            n_svs[n1] = len(self.sv_coeffs[n1])\n",
    "            \n",
    "        np.savetxt(\"n_svs.dat\",n_svs,'%d')\n",
    "            \n",
    "    def pynq_buffer_init(self):\n",
    "        # DECLARE MEMORY FOR HEAP - need to use lists as there are multiple buffers needing to be transferred simultaneously \n",
    "        # containing different training sets\n",
    "        \n",
    "        self.training_matrix_buffers = []\n",
    "        self.alpha_out_buffers = []\n",
    "        self.output_details_buffers = []\n",
    "        self.kkt_violation_buffers = []\n",
    "        \n",
    "        self.input_details_dpm_buffers = []\n",
    "        \n",
    "        self.input_details_buffer = allocate(shape=(5,), dtype=np.int32)\n",
    "        \n",
    "        for n1 in range(self.no_cores):\n",
    "            self.output_details_buffer = allocate(shape=(1,), dtype=np.uint32)\n",
    "            # declare buffers to receive indication signals to send new copies of training and dot product matrices\n",
    "            # kkt_violation_buffer checks if there is a kkt violation and we need to execute the p loop\n",
    "            self.kkt_violation_buffer = allocate(shape=(1,), dtype=np.uint8)\n",
    "            self.input_details_dpm_buffer = allocate(shape=(2,), dtype=np.uint16)\n",
    "            \n",
    "            self.output_details_buffers.append(self.output_details_buffer)\n",
    "            self.kkt_violation_buffers.append(self.kkt_violation_buffer)\n",
    "            self.input_details_dpm_buffers.append(self.input_details_dpm_buffer)\n",
    "        \n",
    "    def pynq_buffer_delete(self):\n",
    "        # close buffers - clean up heap memory\n",
    "        \n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            self.training_matrix_buffers[n1].close()\n",
    "            self.input_details_buffer.close()\n",
    "            \n",
    "            self.alpha_out_buffers[n1].close()\n",
    "            self.output_details_buffers[n1].close()\n",
    "        \n",
    "    def SMO_parallel(self, training_matrices, training_labels, index_1, index_2, no_training_vectors, no_variables, C, tolerance, max_itr):\n",
    "        # index_1 is the index of the first training vector with respoect to the entire training dataset\n",
    "        # index_2 is same but for negative class\n",
    "        # lists for the different parallel classifier executions\n",
    "        \n",
    "        # initialise buffers\n",
    "        self.pynq_buffer_init()\n",
    "        no_variables = int(no_variables)\n",
    "        \n",
    "        # store start index of negative class\n",
    "        negative_class_index = np.zeros(shape=(self.no_cores), dtype=np.uint32)\n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            negative_class_index[n1] = np.where(training_labels[n1] == -1)[0][0]\n",
    "            \n",
    "        # instantiate all DMAs\n",
    "        for n1 in range(self.no_cores):\n",
    "            # check if we are out of range of no_classifiers\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            self.dma_dp_o_1.append(self.SMO_dispatcher[n1+1].dma_dp_o_1)\n",
    "            self.dma_dp_i_1.append(self.SMO_dispatcher[n1+1].dma_dp_i_1)\n",
    "            self.dma_dp_o_2.append(self.SMO_dispatcher[n1+1].dma_dp_o_2)\n",
    "            self.dma_dp_i_2.append(self.SMO_dispatcher[n1+1].dma_dp_i_2)\n",
    "            self.dma_dp_id_o.append(self.SMO_dispatcher[n1+1].dma_dp_id_o)\n",
    "            self.dma_dp_id_i.append(self.SMO_dispatcher[n1+1].dma_dp_id_i)\n",
    "            self.dma_tl.append(self.SMO_dispatcher[n1+1].dma_tl)\n",
    "            self.dma_tm_o.append(self.SMO_dispatcher[n1+1].dma_tm_o)\n",
    "            self.dma_tm_i.append(self.SMO_dispatcher[n1+1].dma_tm_i)\n",
    "            self.dma_id.append(self.SMO_dispatcher[n1+1].dma_id)\n",
    "            self.dma_ao.append(self.SMO_dispatcher[n1+1].dma_ao)\n",
    "            self.dma_kkt.append(self.SMO_dispatcher[n1+1].dma_kkt)\n",
    "            self.dma_od.append(self.SMO_dispatcher[n1+1].dma_od)\n",
    "            \n",
    "        # allocate buffers for each classifier\n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            no_training_vectors[n1] = int(no_training_vectors[n1])\n",
    "            self.training_matrix_buffer = allocate(shape=(no_training_vectors[n1]*no_variables,), dtype=np.uint16)\n",
    "            self.alpha_out_buffer = allocate(shape=(no_training_vectors[n1],1), dtype=np.uint32)\n",
    "            \n",
    "            self.training_matrix_buffers.append(self.training_matrix_buffer)\n",
    "            self.alpha_out_buffers.append(self.alpha_out_buffer)\n",
    "                \n",
    "        # loop over number of cores to send data which is constant for the current classifier\n",
    "        for n1 in range(self.no_cores):\n",
    "            if(n1 == len(self.classifier_indices)):\n",
    "                break\n",
    "            training_labels_buffer = allocate(shape=(no_training_vectors[n1],), dtype=np.int8)\n",
    "            np.copyto(training_labels_buffer, training_labels[n1])\n",
    "            \n",
    "            # also want to obtain and store the training matrices and dot product mastrices for all classifiers in this parallel iteration\n",
    "            np.copyto(self.training_matrix_buffers[n1], training_matrices[n1])\n",
    "\n",
    "            self.no_training_vectors_all[n1] = no_training_vectors[n1]\n",
    "            \n",
    "            #### INPUT DETAILS\n",
    "            # convert floats to IEEE754 bits format\n",
    "            no_training_vectors_IEEE754 = np.asarray(no_training_vectors[n1], dtype=np.float32).view(np.int32).item()\n",
    "            no_variables_IEEE754 = np.asarray(no_variables, dtype=np.float32).view(np.int32).item()\n",
    "            max_itr_IEEE754 = np.asarray(max_itr, dtype=np.float32).view(np.int32).item()\n",
    "            tol_IEEE754 = np.asarray(tolerance, dtype=np.float32).view(np.int32).item()\n",
    "            C_IEEE754 = np.asarray(C, dtype=np.float32).view(np.int32).item()\n",
    "                    \n",
    "            # INPUT_DETAILS\n",
    "            # no_training_vectors - from file\n",
    "            # no_variables - from file\n",
    "            # max_itr, tolerance, C - specified by user\n",
    "            x = [no_training_vectors_IEEE754, no_variables_IEEE754, max_itr_IEEE754, tol_IEEE754, C_IEEE754]\n",
    "            np.copyto(self.input_details_buffer, np.asarray(x, np.int32))\n",
    "            \n",
    "            # INPUT_DETAILS for matrix multiply cores - these are integer, not float\n",
    "            x = [no_training_vectors[n1], no_variables]\n",
    "            np.copyto(self.input_details_dpm_buffers[n1], np.asarray(x, np.uint16))\n",
    "            \n",
    "            # transfer input details and training labels to DMA\n",
    "            # send channels:\n",
    "            self.dma_id[n1].sendchannel.transfer(self.input_details_buffer)\n",
    "            self.dma_tl[n1].sendchannel.transfer(training_labels_buffer)\n",
    "            self.dma_id[n1].sendchannel.wait()\n",
    "            self.dma_tl[n1].sendchannel.wait()\n",
    "            \n",
    "            # receive channels\n",
    "            self.dma_ao[n1].recvchannel.transfer(self.alpha_out_buffers[n1])\n",
    "                        \n",
    "        # if this is 0, the design has exited without changed_alphas = 0 meaning its iterations have saturated\n",
    "        # if it is 1, then we requiured less iterations than specified\n",
    "        # this parameter is used to determing the last element in the \"output_details\" stream - the last element should be the offset\n",
    "        changed_alphas_exit = np.zeros(shape=(self.no_cores,), dtype=np.uint32)\n",
    "                            \n",
    "        # iterate over the maximum number of iterations\n",
    "        for n0 in range(max_itr):\n",
    "            # iterate over the cores\n",
    "            for n0_1 in range(self.no_cores):\n",
    "                if(n0_1 == len(self.classifier_indices)):\n",
    "                    break\n",
    "                if(changed_alphas_exit[n0_1] == 0):\n",
    "                    self.dma_od[n0_1].recvchannel.transfer(self.output_details_buffers[n0_1])\n",
    "\n",
    "                    self.dma_dp_id_o[n0_1].sendchannel.transfer(self.input_details_dpm_buffers[n0_1])\n",
    "                    self.dma_dp_id_o[n0_1].sendchannel.wait()\n",
    "            \n",
    "                    # TRANSFER OUTER DOT PRODUCT MATRIX (FIRST TRAINING MATRIX - NEEDED ONCE PER ITERATION):\n",
    "                    self.dma_dp_o_1[n0_1].sendchannel.transfer(self.training_matrix_buffers[n0_1])\n",
    "                    # send initial copy of outer training matrix and dot product matrix\n",
    "                    self.dma_tm_o[n0_1].sendchannel.transfer(self.training_matrix_buffers[n0_1])\n",
    "        \n",
    "            # p loops:\n",
    "            for n1 in range(max(self.no_training_vectors_all)):\n",
    "                # iterate over the cores\n",
    "                for n1_1 in range(self.no_cores):\n",
    "                    if(n1_1 == len(self.classifier_indices)):\n",
    "                        break\n",
    "                    if(changed_alphas_exit[n1_1] == 0):\n",
    "                        if(n1 < self.no_training_vectors_all[n1_1]):\n",
    "                            \n",
    "                            # loop until we see a kkt violation or can execute next iteration of SMO\n",
    "                            self.dma_kkt[n1_1].recvchannel.transfer(self.kkt_violation_buffers[n1_1])\n",
    "                            \n",
    "                            # TRANSFER SECOND TRAINING MATRIX (TO COMPUTE OUTER DOT PRODUCT MATRIX) - ONCE PER P LOOP\n",
    "                            self.dma_dp_o_2[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "                            self.dma_dp_o_2[n1_1].sendchannel.wait()\n",
    "\n",
    "                            while(1):\n",
    "                                s2mm_status_kkt = self.dma_kkt[n1_1].read(0x34)\n",
    "                                if(s2mm_status_kkt == 4098):\n",
    "                                    break\n",
    "                    \n",
    "                            if(self.kkt_violation_buffers[n1_1] == 1):                                \n",
    "                                # transfer nput details for matrix multiply core\n",
    "                                self.dma_dp_id_i[n1_1].sendchannel.transfer(self.input_details_dpm_buffers[n1_1])\n",
    "                                self.dma_dp_id_i[n1_1].sendchannel.wait()\n",
    "                                # kkt violation - transfer inner matrices\n",
    "                                self.dma_tm_i[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "\n",
    "                                # COMPUTE AND TRANSFER DOT PRODUCT MATRIX:\n",
    "                                self.dma_dp_i_1[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "                                for n1_2 in range(self.no_training_vectors_all[n1_1]):\n",
    "                                    self.dma_dp_i_2[n1_1].sendchannel.transfer(self.training_matrix_buffers[n1_1])\n",
    "                                    self.dma_dp_i_2[n1_1].sendchannel.wait()\n",
    "                                    \n",
    "                                self.dma_tm_i[n1_1].sendchannel.wait()\n",
    "                                self.dma_dp_i_1[n1_1].sendchannel.wait()\n",
    "            \n",
    "            # check to see if we should go to next iteration or if alpha has been calculated - we can break\n",
    "            # if we should go to next iteration\n",
    "            # loop over no_cores:\n",
    "            for n0_1 in range(self.no_cores):\n",
    "                if(n0_1 == len(self.classifier_indices)):\n",
    "                    break\n",
    "                if(changed_alphas_exit[n0_1] == 0):\n",
    "                    while(1):\n",
    "                        s2mm_status_od = self.dma_od[n0_1].read(0x34)\n",
    "                        if(s2mm_status_od == 4098):\n",
    "                            test = int(self.output_details_buffers[n0_1])\n",
    "                            break\n",
    "                    \n",
    "                    if(int(self.fixed_point_to_float(test,32,12)) == (n0 + 1)):\n",
    "                        # exiting with \"changed_alphas =/= 0\" (on last iteration)\n",
    "                        changed_alphas_exit[n0_1] = 0\n",
    "                    else:\n",
    "                        # exiting with \"changed_alphas == 0\"\n",
    "                        changed_alphas_exit[n0_1] = 1\n",
    "            \n",
    "            # check if all training models are completed - if any remain, continue\n",
    "            # if all complete, break_all goes to 1 - can read results from all classifiers\n",
    "            break_all = 0\n",
    "            for n0_1 in range(self.no_cores):\n",
    "                if(n0_1 == len(self.classifier_indices)):\n",
    "                    break\n",
    "                if(changed_alphas_exit[n0_1] == 0):\n",
    "                    break\n",
    "                if(n0_1 == (self.no_cores - 1)):\n",
    "                    break_all = 1\n",
    "            \n",
    "            if(break_all == 1):\n",
    "                break\n",
    "            \n",
    "        # loop over cores - get results\n",
    "        for n0 in range(self.no_cores):\n",
    "            if(n0 == len(self.classifier_indices)):\n",
    "                break\n",
    "            if(changed_alphas_exit[n0] == 0):\n",
    "                self.dma_od[n0].recvchannel.transfer(self.output_details_buffers[n0])\n",
    "                self.dma_od[n0].recvchannel.wait()\n",
    "        \n",
    "            # DMA wait\n",
    "            self.dma_ao[n0].recvchannel.wait()\n",
    "            \n",
    "            # NEED TO OBTAIN COEFFICIENTS - ALPHAS OF NEGATIVE CLASS SHOULD BE MULTIPLIED BY -1\n",
    "            coeffs_temp = self.alpha_out_buffers[n0]\n",
    "            coeffs_temp[negative_class_index[n0]:no_training_vectors[n0],0] = coeffs_temp[negative_class_index[n0]:no_training_vectors[n0],0] * -1\n",
    "            # set very small coefficients to zero\n",
    "            coeffs_temp[np.where(np.absolute(coeffs_temp) < 0.00001)] = 0\n",
    "            coeffs_temp_2 = np.zeros(shape=(len(np.where(coeffs_temp != 0)[0])), dtype=np.uint32)\n",
    "            coeffs_temp_2 = coeffs_temp[np.where(coeffs_temp != 0)[0]]\n",
    "            self.sv_coeffs.append(coeffs_temp_2)\n",
    "            self.offsets.append(self.output_details_buffers[n0])\n",
    "            \n",
    "            # GET INDICES OF SUPPORT VECTORS WITH RESPECT TO ENTIRE TRAINING SET\n",
    "            sv_indices_old = np.where(coeffs_temp != 0)[0]\n",
    "            \n",
    "            length_class_1 = negative_class_index[n0]\n",
    "            length_class_2 = no_training_vectors[n0] - length_class_1\n",
    "            \n",
    "            first_classifier_indices = np.where(sv_indices_old < length_class_1)[0]\n",
    "            second_classifier_indices = np.where(sv_indices_old >= length_class_1)[0]\n",
    "            \n",
    "            sv_indices_new = np.zeros(shape=(len(sv_indices_old)), dtype=np.uint8)\n",
    "            \n",
    "            # these lines get the actual indices corresponding to the support vectors identified from the binary training\n",
    "            sv_indices_new[first_classifier_indices] = sv_indices_old[first_classifier_indices] + index_1[n0]\n",
    "            sv_indices_new[second_classifier_indices] = sv_indices_old[second_classifier_indices] - length_class_1 + index_2[n0]\n",
    "            \n",
    "            self.sv_indices.append(sv_indices_new)\n",
    "        \n",
    "        self.pynq_buffer_delete()\n",
    "        \n",
    "    def SMO_driver_top(self, C, tolerance, max_itr):\n",
    "        # this function calls the \"SMO_parallel\" driver function to execute (no_cores) runs of the SMO in parallel\n",
    "        # it populates the \"classifier_instances\" variable with the relevant numbers - e.g. if we had 5 classifiers,\n",
    "        # and 2 cores, \"classifier_instances\" would take the values [1,2] on the first iteration, [3,4] on the second\n",
    "        # and [5] on the third\n",
    "        \n",
    "        # this function also gets the indices for each classifier\n",
    "        # e.g. the labels need to be changed to +1 and -1 and the correct classes of the full training matrix need to be used\n",
    "        \n",
    "        # THIS LIST CONTAINS TRAINING MATRICES AND TRAINING LABELS\n",
    "        training_matrices = []\n",
    "        training_labels = []\n",
    "        no_training_vectors_all = []\n",
    "        training_data_1_ind_all = []\n",
    "        training_data_2_ind_all = []\n",
    "        \n",
    "        # keep track of which classifiers we are working on\n",
    "        self.classifier_indices = []\n",
    "        \n",
    "        self.get_training_matrix()\n",
    "        self.get_training_labels()\n",
    "        self.get_input_details()\n",
    "        \n",
    "        done = 0\n",
    "        current_classifier = 0\n",
    "        \n",
    "        # this keeps track of what core we are currently generating data for - if all cores have been used or more\n",
    "        # need to be used...\n",
    "        core_count = 0\n",
    "        \n",
    "        no_classifiers = int(0.5 * self.no_classes * (self.no_classes - 1))\n",
    "        print(\"no_classifiers -> \", no_classifiers)\n",
    "        \n",
    "        for n1 in range(self.no_classes):\n",
    "            # loop from zero to (no_classes - 1)\n",
    "            for n2 in range(n1 + 1, self.no_classes):\n",
    "                # loop from (upper loop index + 1) to (no_classes - 1)\n",
    "                                       \n",
    "                # iterate over number of cores\n",
    "                if(core_count < self.no_cores and current_classifier < no_classifiers):\n",
    "                    training_data_1_indices = np.where(self.training_labels_data_fi_uint8 == (n1+1))[0]        # postive class\n",
    "                    training_data_2_indices = np.where(self.training_labels_data_fi_uint8 == (n2+1))[0]        # negative class\n",
    "                    training_data_1_ind_all.append(training_data_1_indices[0])\n",
    "                    training_data_2_ind_all.append(training_data_2_indices[0])\n",
    "                    \n",
    "                    length_class_1 = len(training_data_1_indices)\n",
    "                    length_class_2 = len(training_data_2_indices)\n",
    "                    no_training_vectors = length_class_1 + length_class_2\n",
    "                \n",
    "                    # populate the new training matrix with the two classes in question\n",
    "                    # as training matrix has 2 dimensions, find first and last elements of interest\n",
    "                    first_index_1 = training_data_1_indices[0] * self.no_variables_int\n",
    "                    last_index_1 = (training_data_1_indices[length_class_1 - 1] + 1) * self.no_variables_int\n",
    "                    first_index_2 = training_data_2_indices[0] * self.no_variables_int\n",
    "                    last_index_2 = (training_data_2_indices[length_class_2 - 1] + 1) * self.no_variables_int\n",
    "                    \n",
    "                    training_matrix_new = np.zeros(shape=(no_training_vectors*self.no_variables_int), dtype=np.uint16)\n",
    "                    training_matrix_new[0:length_class_1*self.no_variables_int] = self.training_mat_data_fi_uint16[first_index_1:last_index_1]\n",
    "                    training_matrix_new[length_class_1*self.no_variables_int:(length_class_1*self.no_variables_int+length_class_2*self.no_variables_int)] = self.training_mat_data_fi_uint16[first_index_2:last_index_2]\n",
    "                \n",
    "                    # populate training labels with 1s and -1s\n",
    "                    training_labels_new = np.zeros(shape=(no_training_vectors), dtype=np.int8)\n",
    "                    training_labels_new[0:length_class_1] = 1\n",
    "                    training_labels_new[length_class_1:no_training_vectors] = -1\n",
    "                \n",
    "                    training_matrices.append(training_matrix_new)\n",
    "                    training_labels.append(training_labels_new)\n",
    "                    \n",
    "                    no_training_vectors_all.append(no_training_vectors)\n",
    "                \n",
    "                    current_classifier = current_classifier + 1\n",
    "                    core_count = core_count + 1\n",
    "                    \n",
    "                    self.classifier_indices.append(current_classifier)\n",
    "                    \n",
    "                print(\"current_classifier -> \", current_classifier)\n",
    "                    \n",
    "                if(core_count == self.no_cores or current_classifier == no_classifiers):   \n",
    "                    self.SMO_parallel(training_matrices, training_labels, training_data_1_ind_all, training_data_2_ind_all, no_training_vectors_all, self.no_variables_int, C, tolerance, max_itr)\n",
    "                    core_count = 0\n",
    "                    \n",
    "                    # reset lists to empty for next parallel iteration\n",
    "                    training_matrices = []\n",
    "                    training_labels = []\n",
    "                    no_training_vectors_all = []\n",
    "                    training_data_1_ind_all = []\n",
    "                    training_data_2_ind_all = []\n",
    "                    \n",
    "                    # reset this to empty\n",
    "                    self.classifier_indices = []\n",
    "        \n",
    "print(\"...driver loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compute the Training Model\n",
    "\n",
    "We now instantiate the driver and run the top-level function to compute the training model which will consist of 3 binary training models. Three other values are set: $C$ which is the SVM \"cost parameter\", the tolerance which is another SVM parameter and the maximum number of iterations.\n",
    "\n",
    "Since we are using linear SVM, we can calculate the plane equations and plot them. I have also implemented non-linear SVM, using the radial basis function (RBF) kernel, for the SVM deployment and this is discussed in the results section.\n",
    "\n",
    "This driver abstracts all complex functionality into a simple function call for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate training driver class\n",
    "SMO_driver_inst = SMO_driver()\n",
    "\n",
    "# call top-level driver function and set some default parameters\n",
    "C = 10\n",
    "tolerance = 0.0001\n",
    "max_itr = 10\n",
    "\n",
    "SMO_driver_inst.SMO_driver_top(C, tolerance, max_itr)\n",
    "\n",
    "# write training models to files for SVM deployment to use to classify unknown data\n",
    "for n1 in range(SMO_driver_inst.no_classifiers):\n",
    "    SMO_driver_inst.write_training_model_to_files(n1+1)\n",
    "SMO_driver_inst.write_n_svs_to_file()\n",
    "\n",
    "# get the plane equations i.e. the weights vectors and offsets for the 3 binary training models\n",
    "# 1)\n",
    "\n",
    "classifier_select = 1\n",
    "coeffs_1 = []\n",
    "indices_1 = []\n",
    "\n",
    "offset_1 = SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.offsets[classifier_select-1]),32,12)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    coeffs_1.append(SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.sv_coeffs[classifier_select-1][loop]),32,12))\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    indices_1.append(SMO_driver_inst.sv_indices[classifier_select-1][loop])\n",
    "    \n",
    "svs_write = np.reshape(SMO_driver_inst.training_mat_data_fi_uint16,((int(len(SMO_driver_inst.training_mat_data_fi_uint16)/SMO_driver_inst.no_variables_int),SMO_driver_inst.no_variables_int)))\n",
    "svs_write = svs_write[indices_1]\n",
    "\n",
    "svs_1 = np.zeros(shape=(50,3),dtype=np.float32)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])): \n",
    "    for loop_2 in range(SMO_driver_inst.no_variables_int):\n",
    "        svs_1[loop][loop_2] = SMO_driver_inst.fixed_point_to_float(int(svs_write[loop][loop_2]),16,1)\n",
    "    \n",
    "# 2)\n",
    "\n",
    "classifier_select = 2\n",
    "coeffs_2 = []\n",
    "indices_2 = []\n",
    "\n",
    "offset_2 = SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.offsets[classifier_select-1]),32,12)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    coeffs_2.append(SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.sv_coeffs[classifier_select-1][loop]),32,12))\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    indices_2.append(SMO_driver_inst.sv_indices[classifier_select-1][loop])\n",
    "\n",
    "svs_write = np.reshape(SMO_driver_inst.training_mat_data_fi_uint16,((int(len(SMO_driver_inst.training_mat_data_fi_uint16)/SMO_driver_inst.no_variables_int),SMO_driver_inst.no_variables_int)))\n",
    "svs_write = svs_write[indices_2]\n",
    "\n",
    "svs_2 = np.zeros(shape=(50,3),dtype=np.float32)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])): \n",
    "    for loop_2 in range(SMO_driver_inst.no_variables_int):\n",
    "        svs_2[loop][loop_2] = SMO_driver_inst.fixed_point_to_float(int(svs_write[loop][loop_2]),16,1)\n",
    "\n",
    "# 3)\n",
    "\n",
    "classifier_select = 3\n",
    "coeffs_3 = []\n",
    "indices_3 = []\n",
    "\n",
    "offset_3 = SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.offsets[classifier_select-1]),32,12)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    coeffs_3.append(SMO_driver_inst.fixed_point_to_float(int(SMO_driver_inst.sv_coeffs[classifier_select-1][loop]),32,12))\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    indices_3.append(SMO_driver_inst.sv_indices[classifier_select-1][loop])\n",
    "    \n",
    "svs_write = np.reshape(SMO_driver_inst.training_mat_data_fi_uint16,((int(len(SMO_driver_inst.training_mat_data_fi_uint16)/SMO_driver_inst.no_variables_int),SMO_driver_inst.no_variables_int)))\n",
    "svs_write = svs_write[indices_3]\n",
    "\n",
    "svs_3 = np.zeros(shape=(50,3),dtype=np.float32)\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])): \n",
    "    for loop_2 in range(SMO_driver_inst.no_variables_int):\n",
    "        svs_3[loop][loop_2] = SMO_driver_inst.fixed_point_to_float(int(svs_write[loop][loop_2]),16,1)\n",
    "        \n",
    "classifier_select = 1\n",
    "weights_1 = [0,0,0]\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    weights_1 = weights_1 + coeffs_1[loop] * svs_1[loop]\n",
    "    \n",
    "classifier_select = 2\n",
    "weights_2 = [0,0,0]\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    weights_2 = weights_2 + coeffs_2[loop] * svs_2[loop]\n",
    "    \n",
    "classifier_select = 3\n",
    "weights_3 = [0,0,0]\n",
    "\n",
    "for loop in range(len(SMO_driver_inst.sv_coeffs[classifier_select-1])):\n",
    "    weights_3 = weights_3 + coeffs_3[loop] * svs_3[loop]\n",
    "    \n",
    "# plot planes on training data\n",
    "x = np.arange(-1, 1, 0.5)\n",
    "y = np.arange(-1, 1, 0.5)\n",
    "\n",
    "XX, YY = np.meshgrid(x,y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(training_plot_data_x_1, training_plot_data_y_1, training_plot_data_z_1, c='red', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_2, training_plot_data_y_2, training_plot_data_z_2, c='green', s=1, alpha=1)\n",
    "ax.scatter(training_plot_data_x_3, training_plot_data_y_3, training_plot_data_z_3, c='blue', s=1, alpha=1)\n",
    "\n",
    "Z = -(weights_1[0] * XX + weights_1[1] * YY + offset_1) / weights_1[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='yellow')\n",
    "Z = (-weights_2[0] * XX - weights_2[1] * YY - offset_2) * 1./ weights_2[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='magenta')\n",
    "Z = (-weights_3[0] * XX - weights_3[1] * YY - offset_3) * 1./ weights_3[2]\n",
    "ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='cyan')\n",
    "\n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_zlim((-1,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text indicates that we are processing two binary training models in parallel (1 and 2 then 3).\n",
    "\n",
    "- The magenta plane separates the red and blue classes.\n",
    "\n",
    "- The cyan plane separates the blue and green classes.\n",
    "\n",
    "- The yellow plane separates the red and green classes.\n",
    "\n",
    "Now when we plot the unknown data in this space, we can classify the points based on their positions relative to these three planes. In general, SVM works with hyperplanes with any higher dimension of space but for this demo I've used 3 dimensions for visualisation purposes.\n",
    "\n",
    "### 5.4 Visualise Testing Dataset\n",
    "\n",
    "This data is technically unknown however we do know the actual classes (in order to check how accurate the prediction will be). So the colours on the following diagram are **not** the predictions - they will be calculated later by the SVM deployment design. As can be seen, the testing data is also noisier than the training data which will introduce some errors in the prediction step. The training model (three planes) have also been plotted on top of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testing_mat_data = get_testing_matrix()\n",
    "testing_plot_data = np.transpose(np.reshape(testing_mat_data,(150,3)))\n",
    "testing_plot_data_new = testing_plot_data.tolist()\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "testing_plot_data_x_1 = [float(i) for i in testing_plot_data_new[0][0:50]]\n",
    "testing_plot_data_x_2 = [float(i) for i in testing_plot_data_new[0][50:100]]\n",
    "testing_plot_data_x_3 = [float(i) for i in testing_plot_data_new[0][100:150]]\n",
    "\n",
    "testing_plot_data_y_1 = [float(i) for i in testing_plot_data_new[1][0:50]]\n",
    "testing_plot_data_y_2 = [float(i) for i in testing_plot_data_new[1][50:100]]\n",
    "testing_plot_data_y_3 = [float(i) for i in testing_plot_data_new[1][100:150]]\n",
    "\n",
    "testing_plot_data_z_1 = [float(i) for i in testing_plot_data_new[2][0:50]]\n",
    "testing_plot_data_z_2 = [float(i) for i in testing_plot_data_new[2][50:100]]\n",
    "testing_plot_data_z_3 = [float(i) for i in testing_plot_data_new[2][100:150]]\n",
    "\n",
    "ax.scatter(testing_plot_data_x_1, testing_plot_data_y_1, testing_plot_data_z_1, c='red', s=1, alpha=1)\n",
    "ax.scatter(testing_plot_data_x_2, testing_plot_data_y_2, testing_plot_data_z_2, c='green', s=1, alpha=1)\n",
    "ax.scatter(testing_plot_data_x_3, testing_plot_data_y_3, testing_plot_data_z_3, c='blue', s=1, alpha=1)\n",
    "\n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_zlim((-1,1))\n",
    "\n",
    "x = np.arange(-1, 1, 0.5)\n",
    "y = np.arange(-1, 1, 0.5)\n",
    "\n",
    "XX, YY = np.meshgrid(x,y)\n",
    "\n",
    "Z = -(weights_1[0] * XX + weights_1[1] * YY + offset_1) / weights_1[2]\n",
    "sf_1 = ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='yellow')\n",
    "Z = (-weights_2[0] * XX - weights_2[1] * YY - offset_2) * 1./ weights_2[2]\n",
    "sf_2 = ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='magenta')\n",
    "Z = (-weights_3[0] * XX - weights_3[1] * YY - offset_3) * 1./ weights_3[2]\n",
    "sf_3 = ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='cyan')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Load the SVM Deployment Bitstream and Driver Class\n",
    "\n",
    "Similarly to training, we load a bitstream to the PL, this time for SVM deployment. We also load a driver class which computes the predictions for multiple binary training models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP load the overlay\n",
    "from pynq import Overlay\n",
    "\n",
    "overlay = Overlay(\"deployment_linear_PYNQ_Z2.bit\")\n",
    "\n",
    "# ref: https://pynq.readthedocs.io/en/v2.5/overlay_design_methodology/overlay_tutorial.html\n",
    "# ref: http://www.fpgadeveloper.com/2018/03/how-to-accelerate-a-python-function-with-pynq.html\n",
    "\n",
    "from pynq import DefaultIP\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# the PARSE_FILES class is instantiated once and the all files required for computing the geometric values and test predictions\n",
    "# may be loaded, stored and (saved - if required)\n",
    "class parse_files():\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        self.no_variables = None\n",
    "        self.no_variables_int = None\n",
    "        self.no_test_vectors = None\n",
    "        self.no_test_vectors_int= None\n",
    "        self.no_classes_int = None\n",
    "        \n",
    "        # other variables and arrays containing details on the training model and testing set\n",
    "        self.n_svs_data_int = None\n",
    "        self.testing_mat_fi_data_uint16 = None\n",
    "        self.testing_labels_data_int = None\n",
    "        \n",
    "        # these are for each classifier and will need updated several times (for each training model)\n",
    "        self.svs_fi_data_uint16 = None\n",
    "        self.coeff_fi_data_uint32 = None\n",
    "        self.offset_fi_data_uint32 = None\n",
    "        \n",
    "        \n",
    "    def get_ds_details(self):\n",
    "        # read in the dataset details\n",
    "        f = open(\"ds_details.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        ds_details_data = contents.split()\n",
    "        x = np.array(ds_details_data)\n",
    "        ds_details_data_uint32 = np.asarray(x,np.uint32)\n",
    "\n",
    "        # no_variables\n",
    "        self.no_variables = ds_details_data_uint32[0]\n",
    "        self.no_variables_int = self.no_variables\n",
    "        # (single-precision floating point 32 bit representation as an integer)\n",
    "        \n",
    "        # no_test_vectors\n",
    "        self.no_test_vectors = ds_details_data_uint32[1]\n",
    "        self.no_test_vectors_int = self.no_test_vectors\n",
    "\n",
    "        # number of classes\n",
    "        self.no_classes_int = ds_details_data_uint32[2]\n",
    "    \n",
    "    def get_no_svs(self):\n",
    "        # read file containing the number of support vectors for each classifier\n",
    "        f = open(\"n_svs.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        n_svs_data = contents.split()\n",
    "        x = np.array(n_svs_data)\n",
    "        self.n_svs_data_int = np.asarray(x,np.uint32)\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "    def get_testing_matrix(self):\n",
    "        f = open(\"test_matrix_fi.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        testing_mat_fi_data = contents.split()\n",
    "        x = np.array(testing_mat_fi_data)\n",
    "        self.testing_mat_fi_data_uint16 = np.asarray(x, np.uint16)\n",
    "        \n",
    "        f.close()\n",
    "                \n",
    "    def get_testing_labels(self):\n",
    "        # for self checking Python tests\n",
    "        f = open(\"test_labels.dat\",\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        testing_labels_data = contents.split()\n",
    "        x = np.array(testing_labels_data)\n",
    "        self.testing_labels_data_int = np.asarray(x, np.uint8)\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "        f = open(\"test_predictions_libsvm.dat\",\"r\")\n",
    "        \n",
    "        contents = f.read()\n",
    "        testing_labels_data = contents.split()\n",
    "        x = np.array(testing_labels_data)\n",
    "        self.test_predictions_libsvm = np.asarray(x, np.uint8)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "    def get_support_vectors(self, current_classifier):\n",
    "        # return support vectors for a particular classifier\n",
    "        file_ext = \".dat\"\n",
    "        svs_file_name = \"svs_fi_\"\n",
    "        svs_file_name_new = svs_file_name + str(current_classifier) + file_ext\n",
    "        \n",
    "        f = open(svs_file_name_new,\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        svs_fi_data = contents.split()\n",
    "        x = np.array(svs_fi_data)\n",
    "        self.svs_fi_data_uint16 = np.asarray(x,np.uint16)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "    def get_sv_coeffs(self, current_classifier):\n",
    "        # store the support vector coefficients for a classifier\n",
    "        file_ext = \".dat\"\n",
    "        coeffs_file_name = \"coeffs_fi_\"\n",
    "        coeffs_file_name_new = coeffs_file_name + str(current_classifier) + file_ext\n",
    "\n",
    "        f = open(coeffs_file_name_new,\"r\")\n",
    "\n",
    "        contents = f.read()\n",
    "        coeffs_fi_data = contents.split()\n",
    "        x = np.array(coeffs_fi_data)\n",
    "        self.coeffs_fi_data_uint32 = np.asarray(x,np.uint32)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "    def get_offset(self, current_classifier):\n",
    "        # store the offset for a classifier\n",
    "        file_ext = \".dat\"        \n",
    "        offset_file_name  = \"offset_fi_\"\n",
    "        offset_file_name_new = offset_file_name + str(current_classifier) + file_ext\n",
    "        \n",
    "        f = open(offset_file_name_new,\"r\")\n",
    "\n",
    "        offset_fi_data = f.read()\n",
    "        self.offset_fi_data_uint32 = np.asarray(offset_fi_data,np.uint32)\n",
    "    \n",
    "        f.close()\n",
    "    \n",
    "# the GEOMETRIC_VALUES_DRIVER class is instantiated once for each \"geometric_values\" IP core\n",
    "# member functions include loading data to IP core AXI-lite slave interfaces for general design\n",
    "# parameters and generating the contigous buffers to transfer through DMA to the AXI stream (AXIS) \n",
    "# ports on the IP\n",
    "# INHERITS FROM PARSE_FILES\n",
    "from pynq import MMIO\n",
    "\n",
    "import pynq.lib.dma\n",
    "\n",
    "from pynq import allocate\n",
    "\n",
    "class deployment_driver(parse_files):\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "             \n",
    "        # current classifier we are calculating the geometric values for\n",
    "        self.current_classifier = None\n",
    "        \n",
    "        self.geometric_values_out = None\n",
    "        \n",
    "        # get parameters from dat files which are general to all classifiers - i.e. not the support vectors, coefficient or offset\n",
    "        self.get_ds_details()\n",
    "        self.get_no_svs()\n",
    "        self.get_testing_matrix()\n",
    "        \n",
    "        # used for parallel processing of geometric values\n",
    "        self.classifier_indices = []\n",
    "        self.no_classifiers = None\n",
    "        \n",
    "        # LISTS\n",
    "        self.dma_data_instances = []#contains the support vectors followed immediately by testing matrix in C standard type contigous memory\n",
    "        self.dma_cf_instances = []\n",
    "        self.dma_ds_instances = []\n",
    "        self.dma_gv_instances = []\n",
    "        \n",
    "        geometric_values_1 = overlay.geometric_values_1\n",
    "        geometric_values_2 = overlay.geometric_values_2\n",
    "        geometric_values_3 = overlay.geometric_values_3\n",
    "        geometric_values_4 = overlay.geometric_values_4\n",
    "        geometric_values_5 = overlay.geometric_values_5\n",
    "        geometric_values_6 = overlay.geometric_values_6\n",
    "\n",
    "        self.g_v_dispatcher = {\n",
    "            1: geometric_values_1,\n",
    "            2: geometric_values_2,\n",
    "            3: geometric_values_3,\n",
    "            4: geometric_values_4,\n",
    "            5: geometric_values_5,\n",
    "            6: geometric_values_6,\n",
    "        }\n",
    "        \n",
    "        self.geometric_values_all = None\n",
    "        self.test_predictions = None\n",
    "        \n",
    "        self.geometric_values_time = 0\n",
    "        self.test_predictions_time = 0\n",
    "        \n",
    "        self.tm_buffer = None\n",
    "        \n",
    "        \n",
    "    def dma_init(self, no_classifiers):\n",
    "        # initialise buffers not required to change on each iteration\n",
    "        \n",
    "        # store all geometric values here\n",
    "        self.geometric_values_all = np.zeros(shape=(self.no_test_vectors_int,int(no_classifiers)), dtype=np.uint32)\n",
    "        self.test_predictions = np.zeros(shape=(self.no_test_vectors_int,), dtype=np.uint8)\n",
    "        \n",
    "        self.tm_buffer = allocate(shape=(self.no_test_vectors_int*self.no_variables_int,), dtype=np.uint16)\n",
    "        np.copyto(self.tm_buffer,self.testing_mat_fi_data_uint16)\n",
    "        \n",
    "    def dma_delete(self):\n",
    "        self.tm_buffer.close()    \n",
    "        \n",
    "    def dma_transfer_parallel(self, no_classifiers):\n",
    "        # instantiate all DMAs - parallel design\n",
    "        for n1 in range(6):\n",
    "            self.dma_data_instances.append(self.g_v_dispatcher[n1+1].dma_data)\n",
    "            self.dma_cf_instances.append(self.g_v_dispatcher[n1+1].dma_cf)\n",
    "            self.dma_ds_instances.append(self.g_v_dispatcher[n1+1].dma_ds)\n",
    "            self.dma_gv_instances.append(self.g_v_dispatcher[n1+1].dma_gv)\n",
    "            \n",
    "        geometric_values_buffer_1 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)\n",
    "        geometric_values_buffer_2 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)\n",
    "        geometric_values_buffer_3 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)\n",
    "        geometric_values_buffer_4 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)    \n",
    "        geometric_values_buffer_5 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)    \n",
    "        geometric_values_buffer_6 = allocate(shape=(self.no_test_vectors_int,1), dtype=np.uint32)    \n",
    "    \n",
    "        geo_values_dispatcher = {\n",
    "            1: geometric_values_buffer_1,\n",
    "            2: geometric_values_buffer_2,\n",
    "            3: geometric_values_buffer_3,\n",
    "            4: geometric_values_buffer_4,\n",
    "            5: geometric_values_buffer_5,\n",
    "            6: geometric_values_buffer_6,\n",
    "        }\n",
    "        \n",
    "        # accumulate with time taken to transfer data to DMA in each classifier\n",
    "        dma_transfer_time = 0\n",
    "                \n",
    "        # iterate over only required classifiers\n",
    "        for n1 in range(6):\n",
    "            if(n1 < len(self.classifier_indices)):\n",
    "                current_classifier = self.classifier_indices[n1]\n",
    "                print(\"current_classifier: \", current_classifier)\n",
    "\n",
    "                # get training model for current classifier to compute geometric values for this classifier\n",
    "                # support vectors:\n",
    "                self.get_support_vectors(current_classifier)\n",
    "                # offset:\n",
    "                self.get_offset(self.classifier_indices[n1])\n",
    "                # support vector coefficients:\n",
    "                self.get_sv_coeffs(self.classifier_indices[n1])\n",
    "\n",
    "                # no_svs is obtained at start from one file\n",
    "                # length of support vectors plus length of testing matrix by 256 variables\n",
    "                svs_length = self.n_svs_data_int[self.classifier_indices[n1]-1] * self.no_variables_int\n",
    "                testing_matrix_length = self.no_test_vectors_int * self.no_variables_int\n",
    "                data_stream_length = svs_length + testing_matrix_length\n",
    "                # length of coeffs plus one (for the offset)\n",
    "                coeffs_stream_length = self.n_svs_data_int[self.classifier_indices[n1]-1] + 1\n",
    "                    \n",
    "                svs_buffer = allocate(shape=(svs_length,), dtype=np.uint16)\n",
    "                coeffs_buffer = allocate(shape=(coeffs_stream_length,), dtype=np.uint32)\n",
    "                ds_buffer = allocate(shape=(3,), dtype=np.uint32)\n",
    "                \n",
    "                np.copyto(svs_buffer,self.svs_fi_data_uint16)\n",
    "                np.copyto(coeffs_buffer[0:coeffs_stream_length-1], self.coeffs_fi_data_uint32)\n",
    "                coeffs_buffer[coeffs_stream_length-1] = self.offset_fi_data_uint32\n",
    "\n",
    "                ds_buffer[0] = self.n_svs_data_int[self.classifier_indices[n1]-1]\n",
    "                ds_buffer[1] = self.no_variables_int\n",
    "                ds_buffer[2] = self.no_test_vectors_int           \n",
    "                \n",
    "                \n",
    "                # transfer to DMA\n",
    "                start_time = time.time()\n",
    "                self.dma_cf_instances[n1].sendchannel.transfer(coeffs_buffer)\n",
    "                self.dma_ds_instances[n1].sendchannel.transfer(ds_buffer)\n",
    "                self.dma_gv_instances[n1].recvchannel.transfer(geo_values_dispatcher[n1+1])\n",
    "                \n",
    "                self.dma_data_instances[n1].sendchannel.transfer(svs_buffer)\n",
    "                self.dma_data_instances[n1].sendchannel.wait()\n",
    "                self.dma_data_instances[n1].sendchannel.transfer(self.tm_buffer)\n",
    "                \n",
    "                dma_transfer_time = dma_transfer_time + time.time() - start_time\n",
    "                \n",
    "                coeffs_buffer.close()\n",
    "                ds_buffer.close()\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        start_time = time.time()\n",
    "                \n",
    "        for n1 in range(6):\n",
    "            if(n1 < len(self.classifier_indices)):\n",
    "                self.dma_data_instances[n1].sendchannel.wait()\n",
    "                self.dma_cf_instances[n1].sendchannel.wait()\n",
    "                self.dma_ds_instances[n1].sendchannel.wait()\n",
    "                self.dma_gv_instances[n1].recvchannel.wait()\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        elapsed_time = time.time() - start_time + dma_transfer_time\n",
    "        self.geometric_values_time = self.geometric_values_time + elapsed_time\n",
    "                \n",
    "        for n1 in range(6):\n",
    "            if(n1 < len(self.classifier_indices)):        \n",
    "                self.geometric_values_all[:,self.classifier_indices[n1]-1] = geo_values_dispatcher[n1+1][:,0]\n",
    "                geo_values_dispatcher[n1+1].close()        \n",
    "    \n",
    "    def geometric_values_driver(self):\n",
    "        # get training model for current classifier to compute geometric values for this classifier\n",
    "        \n",
    "        # generate require classifier indices in an 8-length array - there are currently 8 instances of geometric values\n",
    "        # e.g. [1,2,3,4,5,6,7,8] then [9,10] if more than 8 classifiers or just [1,2,3,4,5,6]\n",
    "\n",
    "        # get no_classifiers\n",
    "        no_classifiers = self.no_classes_int * (self.no_classes_int - 1) / 2\n",
    "        self.no_classifiers = no_classifiers\n",
    "        \n",
    "        self.dma_init(no_classifiers)\n",
    "\n",
    "        current_classifier = 1\n",
    "        done = 0\n",
    "        \n",
    "        while(done == 0):\n",
    "            # generate indices - reset to length zero\n",
    "            init_classifier = current_classifier\n",
    "            # (init is the first classifier for the next batch of parallel processing)\n",
    "            self.classifier_indices = []\n",
    "            for n1 in range(6):\n",
    "                if(current_classifier < (no_classifiers + 1)):\n",
    "                    self.classifier_indices.append(init_classifier + n1)\n",
    "\n",
    "                    current_classifier = current_classifier + 1\n",
    "            \n",
    "            if((current_classifier-1) == int(no_classifiers)):\n",
    "                done = 1\n",
    "            \n",
    "            # call dma transfer - parallel calculate geometric values\n",
    "            self.dma_transfer_parallel(no_classifiers)\n",
    "        \n",
    "        self.dma_delete()\n",
    "        \n",
    "    def test_predictions_driver(self):     \n",
    "        no_classes = self.no_classes_int\n",
    "        no_test_vectors = self\n",
    "\n",
    "        dma_gv = overlay.test_predictions_1.dma_gv\n",
    "        dma_ds = overlay.test_predictions_1.dma_ds\n",
    "        dma_tp = overlay.test_predictions_1.dma_tp\n",
    "\n",
    "        ge_values_buffer = allocate(shape=(self.no_test_vectors,int(self.no_classifiers)), dtype=np.uint32)\n",
    "        dataset_buffer = allocate(shape=(2,1), dtype=np.uint32)\n",
    "\n",
    "        np.copyto(ge_values_buffer,self.geometric_values_all)\n",
    "\n",
    "        dataset_buffer[0] = self.no_classes_int\n",
    "        dataset_buffer[1] = self.no_test_vectors\n",
    "\n",
    "        test_predictions_out_buffer = allocate(shape=(self.no_test_vectors,1), dtype=np.uint8)\n",
    "\n",
    "        start_time = time.time()\n",
    "            \n",
    "        # transfer to DMA\n",
    "        dma_gv.sendchannel.transfer(ge_values_buffer)\n",
    "        dma_ds.sendchannel.transfer(dataset_buffer)\n",
    "        dma_tp.recvchannel.transfer(test_predictions_out_buffer)\n",
    "\n",
    "        dma_gv.sendchannel.wait()\n",
    "        dma_ds.sendchannel.wait()\n",
    "        dma_tp.recvchannel.wait()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        self.test_predictions_time = elapsed_time\n",
    "\n",
    "        self.test_predictions = test_predictions_out_buffer\n",
    "\n",
    "        # delete memory on heap to avoid memory leakage\n",
    "        ge_values_buffer.close()\n",
    "        dataset_buffer.close()\n",
    "        test_predictions_out_buffer.close()\n",
    "        \n",
    "    def get_test_predictions(self):\n",
    "        # get geometric values\n",
    "        self.geometric_values_time = 0\n",
    "        self.test_predictions_time = 0\n",
    "        start_time = time.time()  \n",
    "        \n",
    "        self.geometric_values_driver()\n",
    "        \n",
    "        self.test_predictions_driver()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        #print(\"\\nTIME TOTAL (WITH FILE READS): \", elapsed_time)\n",
    "        \n",
    "        print(\"TIME TO COMPUTE PREDICTIONS: \", self.geometric_values_time + self.test_predictions_time)\n",
    "\n",
    "print(\"...driver loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Determine the Predicted Classes for the Testing Data\n",
    "\n",
    "Similarly to before, we instantiate the driver class and call the top-level function. The testing data and training models (which are read from the files we just created) are used to compute the vector of test predictions; labels corresponding to each of the unknown testing points.\n",
    "\n",
    "It could also be seen that 3 binary training models were being applied in parallel - this is because there are 6 IP in the design, operating in parallel, for computing the \"geometric values\" which are used to determine the predicted class. I also print the time to compute the predictions which I used to analyse the performance of my design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# instantiate SVM deployment driver class\n",
    "deployment_driver_inst = deployment_driver()\n",
    "\n",
    "# call top-level function\n",
    "deployment_driver_inst.get_test_predictions()\n",
    "\n",
    "# check the accuracy of the prediction and simlarity to libsvm result\n",
    "deployment_driver_inst.get_testing_labels()\n",
    "\n",
    "# track errors to compute accuracy of precdiction\n",
    "err_count = 0\n",
    "# track differences to libsvm - this indicates issues with the numerical precision of the algorithm\n",
    "disimilarity_count = 0\n",
    "\n",
    "for i in range(deployment_driver_inst.no_test_vectors_int):\n",
    "    if(deployment_driver_inst.test_predictions[i] != deployment_driver_inst.testing_labels_data_int[i]):\n",
    "        err_count = err_count + 1\n",
    "    if(deployment_driver_inst.test_predictions[i] != deployment_driver_inst.test_predictions_libsvm[i]):\n",
    "        disimilarity_count = disimilarity_count + 1\n",
    "        \n",
    "print(\"\\naccuracy = \", (deployment_driver_inst.no_test_vectors_int - err_count) / deployment_driver_inst.no_test_vectors_int * 100, \"%\")\n",
    "#print(\"similarity = \", (deployment_driver_inst.no_test_vectors_int - disimilarity_count) / deployment_driver_inst.no_test_vectors_int * 100, \"%\")\n",
    "\n",
    "# print test predictions\n",
    "# ref: https://stackoverflow.com/questions/16816013/is-it-possible-to-print-using-different-colors-in-ipythons-notebook\n",
    "\n",
    "from IPython.display import HTML as html_print\n",
    "def cstr(s, color='black'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "\n",
    "print(\"\\nTest predictions:\\n\")\n",
    "html_print(cstr(' '.join([cstr(str(deployment_driver_inst.test_predictions[0:50,0]), 'red'), cstr(str(deployment_driver_inst.test_predictions[50:100,0]), 'green'), cstr(str(deployment_driver_inst.test_predictions[100:150,0]), 'blue')]), color='black'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list indicates the test predictions for the testing data. The red points should be classified as '1', the green classified as '2' and the blue classified as '3'. The red points were classified accurately and the green and blue had some mis-classifications. The accuracy is ~81% which is easily computed knowing the predictions and the *actual* class labels.\n",
    "\n",
    "### 5.7 Plot the Predictions and Interact with the Model\n",
    "\n",
    "The below code prints an interactable graph with buttons to switch on and off various aspects of it. The graph can be quite slow to load - this is due to the small processor in the Zynq 7020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test predictions for plot\n",
    "pred_1_indices = np.where(deployment_driver_inst.test_predictions[:,0] == 1)\n",
    "pred_2_indices = np.where(deployment_driver_inst.test_predictions[:,0] == 2)\n",
    "pred_3_indices = np.where(deployment_driver_inst.test_predictions[:,0] == 3)\n",
    "\n",
    "pred_plot_data_new = np.asarray(testing_plot_data_new)\n",
    "\n",
    "pred_plot_data_x_1 = [float(i) for i in pred_plot_data_new[0][pred_1_indices[0]]]\n",
    "pred_plot_data_x_2 = [float(i) for i in pred_plot_data_new[0][pred_2_indices[0]]]\n",
    "pred_plot_data_x_3 = [float(i) for i in pred_plot_data_new[0][pred_3_indices[0]]]\n",
    "\n",
    "pred_plot_data_y_1 = [float(i) for i in pred_plot_data_new[1][pred_1_indices[0]]]\n",
    "pred_plot_data_y_2 = [float(i) for i in pred_plot_data_new[1][pred_2_indices[0]]]\n",
    "pred_plot_data_y_3 = [float(i) for i in pred_plot_data_new[1][pred_3_indices[0]]]\n",
    "\n",
    "pred_plot_data_z_1 = [float(i) for i in pred_plot_data_new[2][pred_1_indices[0]]]\n",
    "pred_plot_data_z_2 = [float(i) for i in pred_plot_data_new[2][pred_2_indices[0]]]\n",
    "pred_plot_data_z_3 = [float(i) for i in pred_plot_data_new[2][pred_3_indices[0]]]\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_zlim((-1,1))\n",
    "        \n",
    "ax.set_xlabel('1st principle component')\n",
    "ax.set_ylabel('2nd principle component')\n",
    "ax.set_zlabel('3rd principle component')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "class data_plots():\n",
    "    def __init__(self):\n",
    "        self.sf_1 = None\n",
    "        self.sf_2 = None\n",
    "        self.sf_3 = None\n",
    "        \n",
    "        self.train_scatter_1 = None\n",
    "        self.train_scatter_2 = None\n",
    "        self.train_scatter_3 = None\n",
    "\n",
    "        self.test_scatter_1 = None\n",
    "        self.test_scatter_2 = None\n",
    "        self.test_scatter_3 = None\n",
    "        \n",
    "        self.pred_scatter_1 = None\n",
    "        self.pred_scatter_2 = None\n",
    "        self.pred_scatter_3 = None\n",
    "        \n",
    "        # 0 for off, 1 for on\n",
    "        self.planes_on = 0\n",
    "        self.training_on = 0\n",
    "        self.testing_on = 0\n",
    "        self.predictions_on = 0\n",
    "\n",
    "    def plot_widgets(self):\n",
    "        planes_on_off_lbl = widgets.Label(value = \"switch planes on or off: \")\n",
    "        display(planes_on_off_lbl)\n",
    "        planes_on_off_btn = widgets.Button(description = \"planes off\")\n",
    "        display(planes_on_off_btn)\n",
    "        \n",
    "        train_on_off_lbl = widgets.Label(value = \"switch training data on or off: \")\n",
    "        display(train_on_off_lbl)\n",
    "        train_on_off_btn = widgets.Button(description = \"training data off\")\n",
    "        display(train_on_off_btn)\n",
    "        \n",
    "        test_on_off_lbl = widgets.Label(value = \"switch testing data on or off: \")\n",
    "        display(test_on_off_lbl)\n",
    "        test_on_off_btn = widgets.Button(description = \"testing data off\")\n",
    "        display(test_on_off_btn)\n",
    "\n",
    "        pred_on_off_lbl = widgets.Label(value = \"switch predictions on or off: \")\n",
    "        display(pred_on_off_lbl)\n",
    "        pred_on_off_btn = widgets.Button(description = \"predictions off\")\n",
    "        display(pred_on_off_btn)\n",
    "        \n",
    "        def planes_on_off(button_click_arg):\n",
    "            if(self.planes_on == 0):\n",
    "                x = np.arange(-1, 1, 0.5)\n",
    "                y = np.arange(-1, 1, 0.5)\n",
    "\n",
    "                XX, YY = np.meshgrid(x,y)\n",
    "\n",
    "                Z = -(weights_1[0] * XX + weights_1[1] * YY + offset_1) / weights_1[2]\n",
    "                self.sf_1 = ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='yellow')\n",
    "                Z = (-weights_2[0] * XX - weights_2[1] * YY - offset_2) * 1./ weights_2[2]\n",
    "                self.sf_2 = ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='magenta')\n",
    "                Z = (-weights_3[0] * XX - weights_3[1] * YY - offset_3) * 1./ weights_3[2]\n",
    "                self.sf_3 = ax.plot_surface(XX,YY,Z,rstride=1,cstride=1,alpha=0.5,color='cyan')\n",
    "                    \n",
    "                self.planes_on = 1\n",
    "                planes_on_off_btn.description = \"planes on\"\n",
    "                \n",
    "            else:\n",
    "                self.sf_1.remove()\n",
    "                self.sf_2.remove()\n",
    "                self.sf_3.remove()\n",
    "                \n",
    "                self.planes_on = 0\n",
    "                planes_on_off_btn.description = \"planes off\"\n",
    "                                \n",
    "        def training_data_on_off(button_click_arg):\n",
    "            if(self.training_on == 0):\n",
    "                self.train_scatter_1 = ax.scatter(training_plot_data_x_1, training_plot_data_y_1, training_plot_data_z_1, c='red', marker=\"x\", s=16, alpha=1)\n",
    "                self.train_scatter_2 = ax.scatter(training_plot_data_x_2, training_plot_data_y_2, training_plot_data_z_2, c='green', marker=\"x\", s=16, alpha=1)\n",
    "                self.train_scatter_3 = ax.scatter(training_plot_data_x_3, training_plot_data_y_3, training_plot_data_z_3, c='blue', marker=\"x\", s=16, alpha=1)\n",
    "                self.training_on = 1\n",
    "                train_on_off_btn.description = \"training data on\"\n",
    "\n",
    "            else:\n",
    "                self.train_scatter_1.remove()\n",
    "                self.train_scatter_2.remove()\n",
    "                self.train_scatter_3.remove()\n",
    "                self.training_on = 0\n",
    "\n",
    "                train_on_off_btn.description = \"training data off\"\n",
    "                \n",
    "        def testing_data_on_off(button_click_arg):\n",
    "            if(self.testing_on == 0):\n",
    "                self.test_scatter_1 = ax.scatter(testing_plot_data_x_1, testing_plot_data_y_1, testing_plot_data_z_1, c='red', s=3, alpha=1)\n",
    "                self.test_scatter_2 = ax.scatter(testing_plot_data_x_2, testing_plot_data_y_2, testing_plot_data_z_2, c='green', s=3, alpha=1)\n",
    "                self.test_scatter_3 = ax.scatter(testing_plot_data_x_3, testing_plot_data_y_3, testing_plot_data_z_3, c='blue', s=3, alpha=1)\n",
    "                self.testing_on = 1\n",
    "                \n",
    "                test_on_off_btn.description = \"testing data on\"\n",
    "            else:\n",
    "                self.test_scatter_1.remove()\n",
    "                self.test_scatter_2.remove()\n",
    "                self.test_scatter_3.remove()\n",
    "                self.testing_on = 0\n",
    "                \n",
    "                test_on_off_btn.description = \"testing data off\"\n",
    "                \n",
    "        def prediction_data_on_off(button_click_arg):\n",
    "            if(self.predictions_on == 0):\n",
    "                self.pred_scatter_1 = ax.scatter(pred_plot_data_x_1, pred_plot_data_y_1, pred_plot_data_z_1, c='red', s=4, alpha=1)\n",
    "                self.pred_scatter_2 = ax.scatter(pred_plot_data_x_2, pred_plot_data_y_2, pred_plot_data_z_2, c='green', s=4, alpha=1)\n",
    "                self.pred_scatter_3 = ax.scatter(pred_plot_data_x_3, pred_plot_data_y_3, pred_plot_data_z_3, c='blue', s=4, alpha=1)\n",
    "                self.predictions_on = 1\n",
    "                \n",
    "                pred_on_off_btn.description = \"predictions on\"\n",
    "            else:\n",
    "                self.pred_scatter_1.remove()\n",
    "                self.pred_scatter_2.remove()\n",
    "                self.pred_scatter_3.remove()\n",
    "                self.predictions_on = 0\n",
    "                \n",
    "                pred_on_off_btn.description = \"predictions off\"\n",
    "                  \n",
    "        planes_on_off_btn.on_click(planes_on_off)\n",
    "        train_on_off_btn.on_click(training_data_on_off)\n",
    "        test_on_off_btn.on_click(testing_data_on_off)\n",
    "        pred_on_off_btn.on_click(prediction_data_on_off)\n",
    "        \n",
    "data_plots_inst = data_plots()\n",
    "data_plots_inst.plot_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Latency Results\n",
    "\n",
    "As discussed in my introduction, the purpose of this project was to improve the latency of SVM with FPGA-based hardware acceleration. I had three different designs: linear SVM training and linear SVM deployment (which we saw in this demo) and non-linear SVM deployment using the RBF kernel (which was not shown). \n",
    "\n",
    "I also used both the PYNQ-Z2 board, which contains a Zynq 7020, and the ZCU104 board, which contains a Zynq MPSoC, to assess the performance of my 3 designs. The latter has a substantially larger resource capacity in the hardware and a higher performance ARM processor. I also compared my results to running on software using an Intel Core I7 processor with my equivalent algorithms running on MATLAB and the LIBSVM algorithm running on MATLAB; LIBSVM is a widely used SVM library and a benchmark for my design performance.\n",
    "\n",
    "The results are presented on Figure 4:\n",
    "\n",
    "<br>\n",
    "<img src=\"latency_results.png\" length=750 width=1200>\n",
    "<br>\n",
    "<center> <u> Figure 4: Latency Results </u> </center>\n",
    "<br>\n",
    "\n",
    "For training, two tests were carried out:\n",
    "- 1) changing number of training vectors\n",
    "- 2) changing number of variables\n",
    "\n",
    "For deployment (linear and RBF), four tests were carried out:\n",
    "- 1) changing the number of testing vectors\n",
    "- 2) changing the number of training vectors (used to generate the training model)\n",
    "- 3) changing the number of variables\n",
    "- 4) changing the number of classes\n",
    "\n",
    "(and fixing all other parameters)\n",
    "\n",
    "***The good***\n",
    "<br>\n",
    "The latencies were averaged to get the results shown in Figure 4 which show linear deployment being significantly accelerated on hardware, even compared to the industry-standard LIBSVM performance through MATLAB on an I7 processor. RBF deployment performance was also better in most cases. \n",
    "\n",
    "***The bad***\n",
    "<br>\n",
    "Training performance was actually slower in hardware than software and I've identified the reason for this being that the algorithm does not support a streaming architecture - i.e. we need access data more than once and in an arbitrary order. This resulted in a very complex training driver (as you can see above) and a huge amount of data needing to be streamed repeatedly which I believe to be the cause of this.\n",
    "\n",
    "More details can be found in my report.\n",
    "\n",
    "## 7 Conclusion\n",
    "\n",
    "Overall, I'm happy with the functionality and performance of my design and have demonstrated use of parallel processing and re-usabiltity through my hardware design. The software design also contains elements of interactivity which can be used to analyse the results of the hardware SVM training and deployment algorithms.\n",
    "\n",
    "## 8 References\n",
    "[1] Christophe, E; Mailhes, C; Duhamel, P. (2009) Hyperspectral image compression: Adapting SPIHT and EZW to anisotropic 3-D wavelet coding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
